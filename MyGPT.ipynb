{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Vn-GL3Z_T4gw",
        "S4KTw47ST86h",
        "OfUYhbodgP_B",
        "q6rZmKWnUCsf",
        "4MDI_l9CgkaQ",
        "ji86OB-OraaC",
        "zoWJFZSM_8Lf"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMD2j2q2KftMX0PmkupgR+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RNTUNOOB/Tiny_GPT/blob/main/MyGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgITXcNg4Hbd",
        "outputId": "72ee9952-dc0b-4a1a-e251-11e682e7ba59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-18 17:25:53--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-02-18 17:25:53 (18.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 1"
      ],
      "metadata": {
        "id": "Vn-GL3Z_T4gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Simple bigram language model </h1>\n",
        "\n",
        "```\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open('input.txt', 'r', encoding = \"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "len(text)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(chars)\n",
        "# Tokenization,\n",
        "\n",
        "# Alternatives\n",
        "# sentence level encoding\n",
        "# subword encoding\n",
        "# openai uses Tiktoken for tokenization\n",
        "\n",
        "\n",
        "# this is character level tokenizor\n",
        "stoi = {s : i for i,s in enumerate(chars)}\n",
        "itos = {i : s for i,s in enumerate(chars)}\n",
        "encode = lambda s : [stoi[i] for i in s]\n",
        "decode = lambda l : \"\".join([itos[i] for i in l])\n",
        "import torch\n",
        "data = torch.tensor(encode(text))\n",
        "data[:100]\n",
        "n = int(0.9*len(data))\n",
        "train = data[:n]\n",
        "test = data[n:]\n",
        "blocksize = 8 # context length\n",
        "x = train[:blocksize]\n",
        "y = test[1:blocksize + 1]\n",
        "for i in range(1, blocksize):\n",
        "  print(f\"for context of [{train[:i]}], output is {y[i]}\")\n",
        "batchsize = 4\n",
        "torch.manual_seed = 1337\n",
        "\n",
        "def get_batch(split = 0):\n",
        "  data = train if split == 0 else test\n",
        "  ix = torch.randint(len(data)-blocksize, (batchsize,))\n",
        "  x = torch.stack([data[i:i+blocksize] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+blocksize+1] for i in ix])\n",
        "\n",
        "  return x,y\n",
        "\n",
        "xb, yb = get_batch()\n",
        "print(xb)\n",
        "print(yb)\n",
        "for i in range(1,blocksize):\n",
        "  print(f\"for context of [{xb[0][:i]}], output is {yb[0][i]}\")\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      probs = F.softmax(logits, dim = -1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1) # (B,1)\n",
        "      idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
        "\n",
        "    return idx\n",
        "bi_model = BigramLanguageModel(vocab_size)\n",
        "optimizer = torch.optim.AdamW(bi_model.parameters(),lr = 1e-3)\n",
        "batch_size = 32 # for training\n",
        "for _ in range(10000):\n",
        "  X, Y = get_batch()\n",
        "  logits, loss = bi_model(X,Y)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "print(loss.item())\n",
        "text = decode(bi_model.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens= 100)[0].tolist())\n",
        "print(text)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "elWORvbrhnMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 2"
      ],
      "metadata": {
        "id": "S4KTw47ST86h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> simple bigram language model with position embeddings </h1>\n",
        "\n",
        "```\n",
        "##    version with cuda option enabled\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "# hyperparameter\n",
        "\n",
        "blocksize = 8  # context length\n",
        "batchsize = 32\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "eval_iter = 200\n",
        "lr = 1e-2\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed = 1337\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "with open('input.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# this is character level tokenizor\n",
        "stoi = {s: i for i, s in enumerate(chars)}\n",
        "itos = {i: s for i, s in enumerate(chars)}\n",
        "encode = lambda s: [stoi[i] for i in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "#test train split\n",
        "data = torch.tensor(encode(text))\n",
        "n = int(0.9 * len(data))\n",
        "train = data[:n]\n",
        "test = data[n:]\n",
        "def get_batch(split=0):\n",
        "    local_data = train if split == 0 else test\n",
        "    ix = torch.randint(len(local_data) - blocksize, (batchsize,))\n",
        "    ixb = torch.stack([local_data[i:i + blocksize] for i in ix])\n",
        "    iyb = torch.stack([local_data[i + 1:i + blocksize + 1] for i in ix])\n",
        "    ixb, iyb = ixb.to(device), iyb.to(device)\n",
        "    return ixb, iyb\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    bi_model.eval()\n",
        "    for split, split_name in enumerate(['train', 'val']):\n",
        "        losses = torch.zeros(eval_iter)\n",
        "        for k in range(eval_iter):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = bi_model(X, Y)\n",
        "            losses[k] =  loss.item()\n",
        "        out[split_name] = losses.mean()\n",
        "    bi_model.train()\n",
        "    return out\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(blocksize, n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,  T = idx.shape\n",
        "        \n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "\n",
        "        return idx\n",
        "bi_model = BigramLanguageModel()\n",
        "m = bi_model.to(device)\n",
        "optimizer = torch.optim.AdamW(bi_model.parameters(), lr=lr)\n",
        "for iter in tqdm(range(max_iters)):\n",
        "\n",
        "    # every once in a while, we calculate loss\n",
        "    if iter%eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss = {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # generating a sample batch\n",
        "    xb, yb = get_batch()\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = bi_model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "context = torch.zeros((1,1), dtype=torch.long, device = device)\n",
        "print(decode(bi_model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "B,T,C = 4, 8, 2   # Batch, time, Channels\n",
        "X = torch.randn(B,T,C)   # (B, T, C)\n",
        "\n",
        "xbow = torch.zeros(B,T,C)\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = X[b,:t+1]   # (T, C)\n",
        "    xbow[b,t] = torch.mean(xprev, 0)\n",
        "# version 1 :\n",
        "\n",
        "wei = torch.tril(torch.ones(T,T))   # (T,T)\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ X  # (B, T, T) @ (B, T, C) -----> (B, T, C)    ! here B in wei is added by torch to handle matric batch multiplication\n",
        "\n",
        "torch.allclose(xbow, xbow2)\n",
        "# verion 2\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros(T,T)    #  we have created this for sake of calculation,\n",
        "                          #  but imagine this represnts actual weights which gives idea of how much interesting does this tokem fimd other previous tokens\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))   # here we tell model that you can talk to future tokens\n",
        "wei = F.softmax(wei, dim=1)\n",
        "\n",
        "xbow3 = wei @ X  # here we aggregate their values dependening on their interest\n",
        "\n",
        "torch.allclose(xbow, xbow3)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SRAxkQCmhhkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Wei implementation"
      ],
      "metadata": {
        "id": "OfUYhbodgP_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "B,T,C = 4, 8, 32   # Batch, time, Channels\n",
        "X = torch.randn(B,T,C)   # (B, T, C)\n",
        "\n",
        "xbow = torch.zeros(B,T,C)\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = X[b,:t+1]   # (T, C)\n",
        "    xbow[b,t] = torch.mean(xprev, 0)\n",
        "```\n",
        "\n",
        "\n",
        "version 1\n",
        "\n",
        "```\n",
        "wei = torch.tril(torch.ones(T,T))   # (T,T)\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ X  # (B, T, T) @ (B, T, C) -----> (B, T, C)    ! here B in wei is added by torch to handle matric batch multiplication\n",
        "\n",
        "torch.allclose(xbow, xbow2)\n",
        "```\n",
        "\n",
        "version 2:\n",
        "\n",
        "```\n",
        "# verion 2\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros(T,T)    #  we have created this for sake of calculation,\n",
        "                          #  but imagine this represnts actual weights which gives idea of how much interesting does this tokem fimd other previous tokens\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))   # here we tell model that you can talk to future tokens\n",
        "wei = F.softmax(wei, dim=1)\n",
        "\n",
        "xbow2 = wei @ X  # here we aggregate their values dependening on their interest\n",
        "\n",
        "torch.allclose(xbow, xbow3)\n",
        "```\n",
        "\n",
        "version 3:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# version 3\n",
        "\n",
        "head_size = 16\n",
        "lm_key = nn.Linear(C, head_size, bias=False)\n",
        "lm_query = nn.Linear(C, head_size, bias = False)\n",
        "\n",
        "lm_value = nn.Linear(C, head_size, bias = False)\n",
        "\n",
        "key = lm_key(X) # (B, T, 16)\n",
        "query = lm_query(X)  # (B, T, 16)\n",
        "### -->           wei = query @ key.transpose(-2, -1)  # (B, T, T)\n",
        "\n",
        "# this creates a huge variance, eg.,\n",
        "\n",
        "# print(f'key : {key.var()}, query : {query.var()}, wei : {wei.var()}')\n",
        "\n",
        "'''\n",
        "It is suggested in the attention is everything, to avoid this high variance, we divide wei by sq rt of head_size\n",
        "wei needs to be fairly defused according to paper. if wei has high variance then softmax result of it will be too sharp, narrow and pointy\n",
        "which is basically means it is just looking at single node\n",
        "'''\n",
        "\n",
        "wei = query @ key.transpose(-2, -1) * head_size ** -0.5\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# wei = torch.zeros(T,T)   \n",
        "\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))  \n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "value = lm_value(X)\n",
        "\n",
        "# out = wei @ X\n",
        "out = wei @ value # (B, T, 16)\n",
        "\n",
        "out.shape\n",
        "```"
      ],
      "metadata": {
        "id": "ICEtVWp9gXjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# version 3"
      ],
      "metadata": {
        "id": "q6rZmKWnUCsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> self attention </h1>\n",
        "\n",
        "```\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "# hyperparameter\n",
        "\n",
        "blocksize = 8  # context length\n",
        "batchsize = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iter = 200\n",
        "lr = 1e-3\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed = 1337\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "with open('input.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# this is character level tokenizor\n",
        "stoi = {s: i for i, s in enumerate(chars)}\n",
        "itos = {i: s for i, s in enumerate(chars)}\n",
        "encode = lambda s: [stoi[i] for i in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "#test train split\n",
        "data = torch.tensor(encode(text))\n",
        "n = int(0.9 * len(data))\n",
        "train = data[:n]\n",
        "test = data[n:]\n",
        "def get_batch(split=0):\n",
        "    local_data = train if split == 0 else test\n",
        "    ix = torch.randint(len(local_data) - blocksize, (batchsize,))\n",
        "    ixb = torch.stack([local_data[i:i + blocksize] for i in ix])\n",
        "    iyb = torch.stack([local_data[i + 1:i + blocksize + 1] for i in ix])\n",
        "    ixb, iyb = ixb.to(device), iyb.to(device)\n",
        "    return ixb, iyb\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    bi_model.eval()\n",
        "    for split, split_name in enumerate(['train', 'val']):\n",
        "        losses = torch.zeros(eval_iter)\n",
        "        for k in range(eval_iter):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = bi_model(X, Y)\n",
        "            losses[k] =  loss.item()\n",
        "        out[split_name] = losses.mean()\n",
        "    bi_model.train()\n",
        "    return out\n",
        "# implementing a simple one head of self attention\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(blocksize, blocksize)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C ** -0.5  # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
        "    # wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))     --------> Error to reember for life, fkn hell\n",
        "    wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "\n",
        "    out = wei @ v  # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
        "\n",
        "    return out\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(blocksize, n_embed)\n",
        "        self.sa_head = Head(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,  T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_head(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "          # as we have implemented pos embded. we cannot have idx greater than block size, so must crop it if it bigger\n",
        "          idx_cond = idx[:, -blocksize:]\n",
        "\n",
        "          logits, loss = self(idx_cond)\n",
        "          logits = logits[:, -1, :]  # becomes (B, C)\n",
        "          probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "          idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
        "          idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "\n",
        "        return idx\n",
        "bi_model = BigramLanguageModel()\n",
        "m = bi_model.to(device)\n",
        "optimizer = torch.optim.AdamW(bi_model.parameters(), lr=lr)\n",
        "for iter in tqdm(range(max_iters)):\n",
        "\n",
        "    # every once in a while, we calculate loss\n",
        "    if iter%eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss = {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # generating a sample batch\n",
        "    xb, yb = get_batch()\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = bi_model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "context = torch.zeros((1,1), dtype=torch.long, device = device)\n",
        "print(decode(bi_model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "G_NIV5c-gohh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# version 4"
      ],
      "metadata": {
        "id": "4MDI_l9CgkaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi headed self attention\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "# hyperparameter\n",
        "\n",
        "blocksize = 8  # context length\n",
        "batchsize = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iter = 200\n",
        "lr = 1e-3\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed = 1337\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "with open('input.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# this is character level tokenizor\n",
        "stoi = {s: i for i, s in enumerate(chars)}\n",
        "itos = {i: s for i, s in enumerate(chars)}\n",
        "encode = lambda s: [stoi[i] for i in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "#test train split\n",
        "data = torch.tensor(encode(text))\n",
        "n = int(0.9 * len(data))\n",
        "train = data[:n]\n",
        "test = data[n:]\n",
        "def get_batch(split=0):\n",
        "    local_data = train if split == 0 else test\n",
        "    ix = torch.randint(len(local_data) - blocksize, (batchsize,))\n",
        "    ixb = torch.stack([local_data[i:i + blocksize] for i in ix])\n",
        "    iyb = torch.stack([local_data[i + 1:i + blocksize + 1] for i in ix])\n",
        "    ixb, iyb = ixb.to(device), iyb.to(device)\n",
        "    return ixb, iyb\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    bi_model.eval()\n",
        "    for split, split_name in enumerate(['train', 'val']):\n",
        "        losses = torch.zeros(eval_iter)\n",
        "        for k in range(eval_iter):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = bi_model(X, Y)\n",
        "            losses[k] =  loss.item()\n",
        "        out[split_name] = losses.mean()\n",
        "    bi_model.train()\n",
        "    return out\n",
        "# implementing a simple one head of self attention\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(blocksize, blocksize)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C ** -0.5  # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
        "    # wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))     --------> Error to reember for life, fkn hell\n",
        "    wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "\n",
        "    out = wei @ v  # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
        "\n",
        "    return out\n",
        "class mul_head(nn.Module):\n",
        "\n",
        "  def __init__(self, n_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(blocksize, n_embed)\n",
        "        self.sa_heads = mul_head(4, n_embed//4)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,  T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_heads(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "          # as we have implemented pos embded. we cannot have idx greater than block size, so must crop it if it bigger\n",
        "          idx_cond = idx[:, -blocksize:]\n",
        "\n",
        "          logits, loss = self(idx_cond)\n",
        "          logits = logits[:, -1, :]  # becomes (B, C)\n",
        "          probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "          idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
        "          idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "\n",
        "        return idx\n",
        "bi_model = BigramLanguageModel()\n",
        "m = bi_model.to(device)\n",
        "optimizer = torch.optim.AdamW(bi_model.parameters(), lr=lr)\n",
        "for iter in tqdm(range(max_iters)):\n",
        "\n",
        "    # every once in a while, we calculate loss\n",
        "    if iter%eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss = {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # generating a sample batch\n",
        "    xb, yb = get_batch()\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = bi_model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "context = torch.zeros((1,1), dtype=torch.long, device = device)\n",
        "print(decode(bi_model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mYrpRYK3rdu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 5"
      ],
      "metadata": {
        "id": "ji86OB-OraaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mulit headed self attention with feed forward\n",
        "\n",
        "\n",
        "```\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "# hyperparameter\n",
        "\n",
        "blocksize = 8  # context length\n",
        "batchsize = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iter = 200\n",
        "lr = 1e-3\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed = 1337\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "with open('input.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# this is character level tokenizor\n",
        "stoi = {s: i for i, s in enumerate(chars)}\n",
        "itos = {i: s for i, s in enumerate(chars)}\n",
        "encode = lambda s: [stoi[i] for i in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "#test train split\n",
        "data = torch.tensor(encode(text))\n",
        "n = int(0.9 * len(data))\n",
        "train = data[:n]\n",
        "test = data[n:]\n",
        "def get_batch(split=0):\n",
        "    local_data = train if split == 0 else test\n",
        "    ix = torch.randint(len(local_data) - blocksize, (batchsize,))\n",
        "    ixb = torch.stack([local_data[i:i + blocksize] for i in ix])\n",
        "    iyb = torch.stack([local_data[i + 1:i + blocksize + 1] for i in ix])\n",
        "    ixb, iyb = ixb.to(device), iyb.to(device)\n",
        "    return ixb, iyb\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    bi_model.eval()\n",
        "    for split, split_name in enumerate(['train', 'val']):\n",
        "        losses = torch.zeros(eval_iter)\n",
        "        for k in range(eval_iter):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = bi_model(X, Y)\n",
        "            losses[k] =  loss.item()\n",
        "        out[split_name] = losses.mean()\n",
        "    bi_model.train()\n",
        "    return out\n",
        "# implementing a simple one head of self attention\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(blocksize, blocksize)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C ** -0.5  # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
        "    # wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))     --------> Error to reember for life, fkn hell\n",
        "    wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "\n",
        "    out = wei @ v  # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
        "\n",
        "    return out\n",
        "class mul_head(nn.Module):\n",
        "\n",
        "  def __init__(self, n_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "class feed_forward(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, n_embed),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(blocksize, n_embed)\n",
        "        self.sa_heads = mul_head(4, n_embed//4)\n",
        "        self.ffd = feed_forward(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,  T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_heads(x)\n",
        "        x = self.ffd(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "          # as we have implemented pos embded. we cannot have idx greater than block size, so must crop it if it bigger\n",
        "          idx_cond = idx[:, -blocksize:]\n",
        "\n",
        "          logits, loss = self(idx_cond)\n",
        "          logits = logits[:, -1, :]  # becomes (B, C)\n",
        "          probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "          idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
        "          idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "\n",
        "        return idx\n",
        "bi_model = BigramLanguageModel()\n",
        "m = bi_model.to(device)\n",
        "optimizer = torch.optim.AdamW(bi_model.parameters(), lr=lr)\n",
        "for iter in tqdm(range(max_iters)):\n",
        "\n",
        "    # every once in a while, we calculate loss\n",
        "    if iter%eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss = {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # generating a sample batch\n",
        "    xb, yb = get_batch()\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = bi_model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "context = torch.zeros((1,1), dtype=torch.long, device = device)\n",
        "print(decode(bi_model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7SJEa9qgAAQx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Er74Bt0jrWkn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNGWDOtY_vjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# version  6"
      ],
      "metadata": {
        "id": "zoWJFZSM_8Lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we would like to combine self attention and feed forward which translates to we will combine communication and compute <br> <br>\n",
        "communication is done in self attention layers where each token passes its query to every other token in a batch.<br>\n",
        "Compute is done in feed forward layer where each token thinks about whats required and whats present <br><br>\n",
        "Now block class which embodies self attention and feed forward makes neural network more dense and adds depth to it. This creates an optimization harder.\n",
        "Therefore, we use technique from research paper. we skip blocks and it is called <b> residual connections .</b> <br>\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "# hyperparameter\n",
        "\n",
        "blocksize = 8  # context length\n",
        "batchsize = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iter = 200\n",
        "lr = 1e-3\n",
        "n_embed = 32\n",
        "\n",
        "torch.manual_seed = 1337\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "with open('input.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# this is character level tokenizor\n",
        "stoi = {s: i for i, s in enumerate(chars)}\n",
        "itos = {i: s for i, s in enumerate(chars)}\n",
        "encode = lambda s: [stoi[i] for i in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "#test train split\n",
        "data = torch.tensor(encode(text))\n",
        "n = int(0.9 * len(data))\n",
        "train = data[:n]\n",
        "test = data[n:]\n",
        "def get_batch(split=0):\n",
        "    local_data = train if split == 0 else test\n",
        "    ix = torch.randint(len(local_data) - blocksize, (batchsize,))\n",
        "    ixb = torch.stack([local_data[i:i + blocksize] for i in ix])\n",
        "    iyb = torch.stack([local_data[i + 1:i + blocksize + 1] for i in ix])\n",
        "    ixb, iyb = ixb.to(device), iyb.to(device)\n",
        "    return ixb, iyb\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    bi_model.eval()\n",
        "    for split, split_name in enumerate(['train', 'val']):\n",
        "        losses = torch.zeros(eval_iter)\n",
        "        for k in range(eval_iter):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = bi_model(X, Y)\n",
        "            losses[k] =  loss.item()\n",
        "        out[split_name] = losses.mean()\n",
        "    bi_model.train()\n",
        "    return out\n",
        "# implementing a simple one head of self attention\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(blocksize, blocksize)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C ** -0.5  # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
        "    # wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))     --------> Error to reember for life, fkn hell\n",
        "    wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "\n",
        "    out = wei @ v  # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
        "\n",
        "    return out\n",
        "class mul_head(nn.Module):\n",
        "\n",
        "  def __init__(self, n_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
        "    self.proj = nn.Linear(n_heads * head_size, n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    return self.proj(out)\n",
        "class feed_forward(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4 * n_embed),  # here, according to paper, expected output is 4 times the input. so we nulitply it by 8\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embed, n_embed)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embed, n_heads):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_heads\n",
        "    self.sa = mul_head(n_heads, head_size)\n",
        "    self.ffd = feed_forward(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(x)\n",
        "    x = x + self.ffd(x)\n",
        "    return x\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(blocksize, n_embed)\n",
        "        self.block = nn.Sequential(\n",
        "            Block(n_embed, n_heads=4),\n",
        "            Block(n_embed, n_heads=4),\n",
        "            Block(n_embed, n_heads=4)\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,  T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.block(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "          # as we have implemented pos embded. we cannot have idx greater than block size, so must crop it if it bigger\n",
        "          idx_cond = idx[:, -blocksize:]\n",
        "\n",
        "          logits, loss = self(idx_cond)\n",
        "          logits = logits[:, -1, :]  # becomes (B, C)\n",
        "          probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "          idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
        "          idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "\n",
        "        return idx\n",
        "bi_model = BigramLanguageModel()\n",
        "m = bi_model.to(device)\n",
        "optimizer = torch.optim.AdamW(bi_model.parameters(), lr=lr)\n",
        "for iter in tqdm(range(max_iters)):\n",
        "\n",
        "    # every once in a while, we calculate loss\n",
        "    if iter%eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss = {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # generating a sample batch\n",
        "    xb, yb = get_batch()\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = bi_model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "context = torch.zeros((1,1), dtype=torch.long, device = device)\n",
        "print(decode(bi_model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "M2iAgIdBBEQM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aENbmnfoATJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 7"
      ],
      "metadata": {
        "id": "7EeHkK1yM6dF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will add layerNorm. It normalizes each row in out batch, so normalizes each token. i.e. it make such that mean of each row is 0 and has std var of 1\n",
        "<br> <br>\n",
        "eg.\n",
        "\n",
        "```\n",
        "class layerNorm:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum = 0.5):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.ones(dim)\n",
        "\n",
        "  def __call__(self):\n",
        "    xmean = x.mean(1, keepdim=True)\n",
        "    xvar = x.var(1, keepdim= True)\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "```\n",
        "\n",
        "<br> <br>\n",
        "\n",
        "according to paper, normalization is done after transformation i.e. after attention and ffd and all that. But in recent time, normalization is done before transformation <br> <br>\n",
        "\n",
        "we also implement dropout here. for regularization. This randomaly shuts down some nodes which acts as a regularization. <br>"
      ],
      "metadata": {
        "id": "J9fI0S0fM8OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "883qWxdpNlxy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter\n",
        "\n",
        "blocksize = 32  # context length\n",
        "batchsize = 64\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iter = 200\n",
        "lr = 3e-3\n",
        "n_embed = 384\n",
        "n_layer = 6\n",
        "n_heads = 6\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed = 1337\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "yV2rZI7Hfc6I"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "6Zyuy9k-fc3v"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# this is character level tokenizor\n",
        "stoi = {s: i for i, s in enumerate(chars)}\n",
        "itos = {i: s for i, s in enumerate(chars)}\n",
        "encode = lambda s: [stoi[i] for i in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "uNGVqWQbfcx3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test train split\n",
        "data = torch.tensor(encode(text))\n",
        "n = int(0.9 * len(data))\n",
        "train = data[:n]\n",
        "test = data[n:]"
      ],
      "metadata": {
        "id": "8xGkwcMqfcvf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split=0):\n",
        "    local_data = train if split == 0 else test\n",
        "    ix = torch.randint(len(local_data) - blocksize, (batchsize,))\n",
        "    ixb = torch.stack([local_data[i:i + blocksize] for i in ix])\n",
        "    iyb = torch.stack([local_data[i + 1:i + blocksize + 1] for i in ix])\n",
        "    ixb, iyb = ixb.to(device), iyb.to(device)\n",
        "    return ixb, iyb"
      ],
      "metadata": {
        "id": "vGQVJ-ZcfuFY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    bi_model.eval()\n",
        "    for split, split_name in enumerate(['train', 'val']):\n",
        "        losses = torch.zeros(eval_iter)\n",
        "        for k in range(eval_iter):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = bi_model(X, Y)\n",
        "            losses[k] =  loss.item()\n",
        "        out[split_name] = losses.mean()\n",
        "    bi_model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "imtPJVqIfuDp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing a simple one head of self attention\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(blocksize, blocksize)))\n",
        "    self.Dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C ** -0.5  # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
        "    # wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))     --------> Error to reember for life, fkn hell\n",
        "    wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "    wei = self.Dropout(wei)\n",
        "\n",
        "    out = wei @ v  # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "4pyPq1MliFtk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class mul_head(nn.Module):\n",
        "\n",
        "  def __init__(self, n_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
        "    self.proj = nn.Linear(n_heads * head_size, n_embed)\n",
        "    self.Dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.Dropout(self.proj(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "s2JTJfTFh-KR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class feed_forward(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4 * n_embed),  # here, according to paper, expected output is 4 times the input. so we nulitply it by 8\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embed, n_embed),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "z8Ru2TtK-ldp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embed, n_heads):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_heads\n",
        "    self.sa = mul_head(n_heads, head_size)\n",
        "    self.ffd = feed_forward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "crnhjprlBvCV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(blocksize, n_embed)\n",
        "        self.block = nn.Sequential(*[Block(n_embed, n_heads = n_heads) for _ in range(n_layer)])\n",
        "        self.lnf = nn.LayerNorm(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,  T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.block(x)\n",
        "        x = self.lnf(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "          # as we have implemented pos embded. we cannot have idx greater than block size, so must crop it if it bigger\n",
        "          idx_cond = idx[:, -blocksize:]\n",
        "\n",
        "          logits, loss = self(idx_cond)\n",
        "          logits = logits[:, -1, :]  # becomes (B, C)\n",
        "          probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "          idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
        "          idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "2515Ao-gfuBZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bi_model = BigramLanguageModel()\n",
        "m = bi_model.to(device)\n",
        "optimizer = torch.optim.AdamW(bi_model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "aPEqPmIKft_C"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in tqdm(range(max_iters)):\n",
        "\n",
        "    # every once in a while, we calculate loss\n",
        "    if iter%eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss = {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # generating a sample batch\n",
        "    xb, yb = get_batch()\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = bi_model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06nmkLFNfctO",
        "outputId": "646577f7-8d1a-4697-c8a7-8014181d1830"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/5000 [00:07<10:45:21,  7.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss = 4.3596, val loss: 4.3548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 503/5000 [00:45<1:07:41,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: train loss = 2.1801, val loss: 2.2176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1003/5000 [01:23<59:16,  1.12it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: train loss = 1.9614, val loss: 2.0595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 1503/5000 [02:00<51:54,  1.12it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1500: train loss = 1.9355, val loss: 2.0466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 2003/5000 [02:38<44:18,  1.13it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2000: train loss = 1.9131, val loss: 2.0263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 2503/5000 [03:15<36:59,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 2500: train loss = 1.9902, val loss: 2.0934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 3003/5000 [03:53<28:29,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3000: train loss = 2.2019, val loss: 2.2608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 3503/5000 [04:30<21:19,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 3500: train loss = 2.2439, val loss: 2.2910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 4001/5000 [05:08<20:29,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 4000: train loss = 2.1627, val loss: 2.2303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 4503/5000 [05:45<07:21,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 4500: train loss = 2.1641, val loss: 2.2254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [06:14<00:00, 13.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device = device)\n",
        "print(decode(bi_model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "QfdvrLBQlz4-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3aebb54-4890-4ea0-a5e4-957e52aaabc6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Wote heriu\n",
            "hhyb hy bi foa nowoorvyetrcks,\n",
            "A\n",
            "tabeesath Iis hise a sha'\n",
            "And Rhay fret walave ast, spotenichapsscroungelhoe wh tisatelk inot gled arathy bee he, or theessearce e E nesea ond saiverd nor coooryte,\n",
            "Chim u I ce heacer prarpor'hy frot pe cal wche \n",
            "Thert watdo afthavll iso ud you\n",
            "Tiar gansodtowpynsndem' an tr tever,\n",
            "Thasdoska anto-bho l'd Mack, hace oferdeere I kiors\n",
            "Ther be forncutthought intl ouboterfee aol alcon eife bo precrom chol'd, Edess lemend,\n",
            "EO,d nown kloogndwire betimeath ed \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BAfjn26-QQDM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j5fVbPvkBjjz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}